# Apache Spark
- An open source big data framework
- Allows to process large volumes of data fast
- It does this by distributing the workload across multiple computers

![](/images/apachespark.PNG)

## What can it do
- Data processing task like (batch processing, streaming, ML learning)
- Fast speed as it processes data in memory rather than writing to disk (Hadoop doesn't)
- Supports Multiple languages like Scala, Java, Python and R

## What its good at
- Speed
- Scalability (workload distributed across servers)
- Versatility (used in many different ways, ML learning, Streaming data, batch processing)
- Open source (source code is made freely and available)

---

# PySpark the Python API for Apache Spark
- Allows you to write spark application using python
- An open source, distributed computing framework

![](/images/pyspark.PNG)

## What can you do with PySpark
- Process large datasets
- Perform data cleaning, transformation and aggregation on big data
- Run SQL queries (Interact with DBs)
- Build and deploy ML models using (MLlib)
- Handle streaming data for real time analytics
- Integrate with Hadoop to build end-to-end data pipelines

---


